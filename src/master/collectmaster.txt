The collection master
=====================

The collection master (`collectmaster`) is the server that listens for
the `ucollect` clients and gathers data from them, saving them to a
database (currently mysql). It consists of a core and several plugins.
Generally, there's a matching plugin for each plugin in `ucollect`.

The core provides some basic features, like listening for the clients,
logging and connecting to the database.

Execution
---------

The server takes exactly one parameter, path to the configuration
file.

  ./collectmaster collect-master.conf

Configuration
-------------

The configuration file is simple ini file (see
http://docs.python.org/2/library/configparser.html, the
`RawConfigParser`). There's one section for the core (called `main`)
and then a section for each plugin. Each plugin section is named by
the fully qualified class name of the plugin (for example
`buckets.main.BucketsPlugin`).

You can find a default configuration file in
`src/master/collect-master.conf`.

All the options are mandatory.

The `main` section
~~~~~~~~~~~~~~~~~~

There are option to configure the core of the collection master.

dbuser::
  User to connect to the database.
dbpasswd::
  Password to authenticate to the database.
db::
  The database name to use.
port::
  The TCP port to use for incoming `ucollect` connections. It listens
  on IPv6 wildcard address, if you need to restrict it to some
  interface or so, use firewall.
log_severity::
  The severity at which log. The values are (with growing severity):
  - TRACE
  - DEBUG
  - INFO
  - WARN
  - ERROR
  - CRITICAL
log_format::
  The format of log messages. Follows the format described at
  http://docs.python.org/2/library/logging.html.
log_file::
  If set to `-`, it logs to standard error output. If it is something
  else, it logs to the given file.

The `count` plugin
~~~~~~~~~~~~~~~~~~

It is in the `count_plugin.CountPlugin` class. There are only few
options, since it mostly stores the received data:

interval::
  Number of seconds between requests for data from clients are sent.
aggregate_delay::
  The request for data is sent. Then the plugin waits this amount of
  seconds. After the wait, what answers came are stored in the
  database as one snapshot. This must be less than interval, otherwise
  strange things will happen.

The `buckets` plugin
~~~~~~~~~~~~~~~~~~~~

The clients hash packets based on some criteria and store their counts
into buckets depending on the hash value. Then, the server aggregates
the client data together and checks the bucket sizes look
statistically similar. If some buckets get out of the line, the
clients are asked to provide keys (the hashed properties of the
packets) for the given buckets and, after aggregating them together,
put to database.

There are therefore many things to configure. This includes
configuration for clients that is sent to them by the plugin:

bucket_count::
  How many buckets there are for single hash function. With too few,
  anomalies might get lost or more keys are attributed to a given
  anomaly. If too many, it might be too sensitive and find too many
  anomalies. Besides, more memory will be used on clients and more
  data sent over network.
hash_count::
  How many independent hash functions are used. An anomaly will be
  anomalous in all the hash functions, therefore by intersecting the
  sets of keys, it is possible to filter out only the relevant keys.
  With more, more data is used, but the probability of unrelated key
  sneaking in the result is smaller.
criteria::
  Whitespace separated list of criterion classes to load and use. A
  criterion extracts is the property of a packet to be hashed.
  Currently available ones are:
  buckets.criterion.Port;; The remote port of TCP/UDP packet.
  buckets.criterion.Address;; The remote address of IP packet.
  buckets.criterion.AddressAndPort;; Both the remote port and address
    of a TCP/UDP packet.
history_size::
  Number of last key snapshots kept in the client's memory. They are
  used to answer the query for keys if an anomaly is found. As the
  server currently asks for the last snapshot only, it is OK to keep
  it low.
max_key_count::
  Limit on number of distinct keys the client keeps in one history
  snapshot per criterion. This is to limit the amount of memory used
  in case when many different keys are found. Such thing would be rare
  in practice and anomalous keys would probably appear early enough to
  be present, so dropping some should not matter.
granularity::
  Each history snapshot is divided into smaller timeslots. Each
  timeslot has separate packet counts in the buckets, but the keys are
  for the whole history snapshot. This is to aid the statistic
  detection of similarity between buckets. The granularity is number
  of seconds of one timeslot.
max_timeslots::
  Number of time slots to allow for one history snapshot. If the
  server does not request the data before the maximum is reached, the
  whole history snapshot will be ignored (since the client is probably
  disconnected, or something). Set this somewhat larger than just
  `interval / granularity`, to allow for some network jitter, stuck
  control connections or so.
interval::
  Number of seconds between the server asks for bucket counts.
gather_history_max::
  How many history snapshots of bucket counts to keep on the server.
  During the statistical analysis, they are concatenated together to
  form longer period of time. The bigger the number, the longer-term
  anomalies are found.
aggregate_delay::
  Number of seconds between sending the request for data and
  processing whatever answers returned. Used both for bucket counts
  and for keys.
anomaly_threshold::
  A magic number, specifying how much different a bucket must be from
  the average to be considered anomalous. With bigger number, only
  more serious anomalies are reported. With smaller, even less serious
  are included, generating more anomalies total.
config_version::
  Increase this number every time you change the config of the plugin.
  This is used to synchronise config with the client.

The database
------------

A mysql database is used by the server.

There's a basic set of tables, and each plugin has it's own additional
set of tables.

Table marks
~~~~~~~~~~~

The tables are flagged to indicate how the server accesses them.

RO::
  Table is not modified by the server. The `SELECT` privilege is
  enough.
WO::
  Table is inserted into, but not read by the server. The `INSERT`
  privilege is enough.
SI::
  Table that is inserted into and selected from by the server. The
  server needs `SELECT` and `INSERT` privileges.

[NOTE]
The current script does not fine-tune permissions. Currently, the
server has full rights on the whole database.

Basic tables
~~~~~~~~~~~~

clients [RO]::
  This table holds existing clients and their login information. It is
  used mostly as reference points from other tables, when they relate
  to clients, but also when checking if a client can connect. This
  table is considered read only for the server.
activity_types [RO]::
  This holds recognized activities of clients. While others are
  expected to appear in future (with more plugins, for example), these
  are currently available:
  login;; Client connected and authenticated with the server.
  logout;; Client disconnected.
  buckets;; Activity on the buckets plugin (some data arrived).
  counts;; Activity on the counts plugin (some data arrived);
activities [WO]::
  This holds records of when each client performed which activity. The
  purpose is both debugging aids and detection of inactive/dead
  routers.

Tables of the `buckets` plugin
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

groups [RO]::
  List of groups to which clients are divided.
group_members [RO]::
  M-N mapping of users to groups (eg. group can contain multiple users
  and user can be in multiple groups).
anomaly_types [RO]::
  This is table of the criteria by which the plugin detects the
  anomalies.
anomalies [WO]::
  This table holds the found anomalies, one key & time per row.
  There's relevance of the key in the table in how many clients of all
  the ones which returned the keys have it.

Tables of the `counts` plugin
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

count_types [RO]::
  List of packet properties being counted.
count_snapshots [SI]::
  A row is stored here for each time and client that sends data. It
  represents one snapshot of the counts.
counts [WO]::
  The actual statistics. Each row are counts for single snapshot and
  type, therefore there are multiple rows for each snapshot.
capture_stats [WO]::
  Statistics for number of captured and dropped packets on a given
  network interface in a given snapshot. There may be multiple
  interfaces in given snapshot.
