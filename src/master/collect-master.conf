[main]
; How to connect to the DB
dbuser: updater
dbpasswd: 12345
db: ucollect
dbhost: localhost
; Port to listen on
port: 5678
; The logging format. See http://docs.python.org/2/library/logging.html
log_format: %(name)s@%(module)s:%(lineno)s	%(asctime)s	%(levelname)s	%(message)s
; Severity of the logs. One of TRACE, DEBUG, INFO, WARN, ERROR, CRITICAL
log_severity: TRACE
; Where to log. - means stderr.
log_file: master.log
; Maximum size of log file (in bytes)
log_file_size: 134217728
; Maximum number of backup log files when rotated
log_file_count: 5
; The SSL certificate
cert = server.cert
key = server.key
; Where the authenticator lives
authenticator_host: localhost
authenticator_port: 8888
fastpings:
	0000000500000842

; The plugins to load follow. Each name is the class to load and instantiate.

[count_plugin.CountPlugin]
; The plugin that counts some stuff (packets of various properties, amount of data, ...)
interval: 60 ; How often to store a snapshot, seconds.
aggregate_delay: 5 ; How long to wait for answers after sending the query to store data into DB.

[buckets.main.BucketsPlugin]
; This one hashes packets to several buckets and checks the bucket sizes look statistically similar.
; If some does not, it is called an anomaly and the packet keys (the hashed properties) are guessed.
bucket_count: 13 ; Number of hash buckets to hash into
hash_count: 5 ; Number of different hash functions, used to guess the right keys
; Whitespace separated list of criteria to use. Each is fully-qualified class name.
criteria: buckets.criterion.AddressAndPort
	buckets.criterion.Port
	buckets.criterion.Address
history_size: 1 ; Number of history snapshots back kept in clients, for asking for keys
; Maximum number of keys kept per history snapshot and criterion on each client.
; Limited so the memory doesn't grow without bounds.
max_key_count: 1000
granularity: 5 ; Number of seconds in each timeslot
max_timeslots: 30 ; Number of time slots allocated in client (if there are more than this during one interval, the client data will be discarded). Recommended is at least twice as much + a little, in case the server would skip a generation.
interval: 60 ; Number of seconds between requesting a snapshot from clients.
gather_history_max: 4 ; Number of snapshots kept back on server, used for computing the anomalies.
aggregate_delay: 5 ; How long to wait for answers from clients between working on them.
anomaly_threshold: 1.8 ; How sensitive to be about anomalies. Lower the number, more the anomalies.
config_version: 1 ; If you change the config, change this as well

[sniff.main.SniffPlugin]
taskers = sniff.cert.Cert
	sniff.ping.Pinger
parallel_limit = 20
task_timeout = 1
interval = 1
# 15 seconds between starting tasks, so they don't flood all at once
start_interval = 15
# 6 hours between ping batches
ping_interval = 21600
cert_interval = 86400

[bandwidth_plugin.BandwidthPlugin]
interval: 900 ; How often to store a snapshot, seconds.
aggregate_delay: 5 ; How long to wait for answers from clients before working on them.

[flow_plugin.FlowPlugin]
version: 1 ; Config version, so client can know when to reload
max_flows: 5000 ; Maximum number of flows kept in memory, if more are needed, they are flushed
timeout: 1800 ; Maximum time to keep the flows unflushed
